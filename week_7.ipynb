{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4a34415",
   "metadata": {},
   "source": [
    "# Context Management Techniques in LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f0d9ba",
   "metadata": {},
   "source": [
    "The workbook includes some of the techniques and tools available to manage context in LangGraph, including:\n",
    "- Checkpointer\n",
    "- Trimming Messages\n",
    "- Summarising Messages\n",
    "- Multi State Schemas\n",
    "- Subgraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c26cd3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from operator import add\n",
    "from IPython.display import display, HTML, Image\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ae0f10",
   "metadata": {},
   "source": [
    "# Short Term Memory\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd9a903",
   "metadata": {},
   "source": [
    "## Checkpointer\n",
    "This section will deomostrate how the checkpointer works.  The checkpoint persists the state of the graph in memory, so that it can be replayed.  A checkpoint can be associated with a thread, so LangGraph can maintain graphs in different states.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "51a788de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples Reducer, to trace when calls happen\n",
    "def my_add(a: int, b: int) -> int:\n",
    "    print(\"Adding:\", a, \"+\", b)\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562ab4de",
   "metadata": {},
   "source": [
    "## Simple Checkpointer Example\n",
    "This example simply writes text to the state (no LLM calls).  It will demonstrate how the checkpointer maintains graph state, and how it can be used to replay from a specific point in the graph.  This is very useful for development and debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b0d9130c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These node functions simply write values to the state.  foo is a string, bar is a list of strings with a reducer that appends new values.\n",
    "class State(BaseModel):\n",
    "    foo: str\n",
    "    bar: Annotated[list[str], my_add]\n",
    "\n",
    "def node_a(state: State):\n",
    "    print(\"Executing Node A\")\n",
    "    return {\"foo\": \"a\", \"bar\": [\"a\"]}\n",
    "\n",
    "def node_b(state: State):\n",
    "    print(\"Executing Node B\")\n",
    "    return {\"foo\": \"b\", \"bar\": [\"b\"]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "cef8a7e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x19e1ea14680>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the workflow graph\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(node_a)\n",
    "workflow.add_node(node_b)\n",
    "workflow.add_edge(START, \"node_a\")\n",
    "workflow.add_edge(\"node_a\", \"node_b\")\n",
    "workflow.add_edge(\"node_b\", END)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673340fa",
   "metadata": {},
   "source": [
    "## Creating and Using the Checkpointer\n",
    "Below the checkpointer is created as an in memory checkpointer (will not be saved between runs - except in Jupyter).\n",
    "\n",
    "The checkpointer is the passed to the graph when it is compiled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c3eec8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checkpointer = InMemorySaver()\n",
    "graph = workflow.compile(checkpointer=checkpointer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64ecb29",
   "metadata": {},
   "source": [
    "### Graph Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f3c1b32c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x0000019E1CCBB130>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d868e8",
   "metadata": {},
   "source": [
    "### Invoking the Graph\n",
    "A checkpointer needs to be associated with a thread when it is run.  To do this we pass the thread into the **config** property of the graph invocation.\n",
    "\n",
    "Note that if you run the cell below multiple times, without recompiling the graph, it starts each new execution with the final state at the end of the previous execution.  The graph state has been persisted by the checkpointer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b5f1096c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding: [] + ['z']\n",
      "Executing Node A\n",
      "Adding: ['z'] + ['a']\n",
      "Executing Node B\n",
      "Adding: ['z', 'a'] + ['b']\n",
      "Final State: {'foo': 'b', 'bar': ['z', 'a', 'b']}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "result = graph.invoke(State(foo=\"foo\", bar=['z']), config)\n",
    "\n",
    "print(\"Final State:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6886534",
   "metadata": {},
   "source": [
    "## Observing State History\n",
    "We can get the final state of the graph stored in the checkpointer with the get_state method, for a thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3685d86e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StateSnapshot(values={'foo': 'b', 'bar': ['z', 'a', 'b']}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0c3284-c463-6c9a-8002-420318b29f90'}}, metadata={'source': 'loop', 'step': 2, 'parents': {}}, created_at='2025-11-16T20:10:29.426493+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0c3284-c463-6168-8001-eb1a03275f4a'}}, tasks=(), interrupts=())"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "graph.get_state(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62622480",
   "metadata": {},
   "source": [
    "The full history of the graph can see viewed using the get_state_history method.  Iterate over this to see see the state of the graph at each at each step in the graph traversal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "53324c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StateSnapshot(values={'foo': 'b', 'bar': ['z', 'a', 'b']}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0c3cf0-4c59-6847-8002-8201850d27dc'}}, metadata={'source': 'loop', 'step': 2, 'parents': {}}, created_at='2025-11-17T16:03:55.419022+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0c3cf0-4c54-6b2b-8001-4b280c0c71a0'}}, tasks=(), interrupts=()) \n",
      "--------------------\n",
      "StateSnapshot(values={'foo': 'a', 'bar': ['z', 'a']}, next=('node_b',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0c3cf0-4c54-6b2b-8001-4b280c0c71a0'}}, metadata={'source': 'loop', 'step': 1, 'parents': {}}, created_at='2025-11-17T16:03:55.417043+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0c3cf0-4c45-6a9f-8000-7f87773528fd'}}, tasks=(PregelTask(id='5af6459f-1c7d-5698-f6dd-22c789089e79', name='node_b', path=('__pregel_pull', 'node_b'), error=None, interrupts=(), state=None, result={'foo': 'b', 'bar': ['b']}),), interrupts=()) \n",
      "--------------------\n",
      "StateSnapshot(values={'foo': 'foo', 'bar': ['z']}, next=('node_a',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0c3cf0-4c45-6a9f-8000-7f87773528fd'}}, metadata={'source': 'loop', 'step': 0, 'parents': {}}, created_at='2025-11-17T16:03:55.410871+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0c3cf0-4c0d-6791-bfff-8f817ecb842f'}}, tasks=(PregelTask(id='feaeb78f-f651-4b0d-ced1-4d7fd38d8b53', name='node_a', path=('__pregel_pull', 'node_a'), error=None, interrupts=(), state=None, result={'foo': 'a', 'bar': ['a']}),), interrupts=()) \n",
      "--------------------\n",
      "StateSnapshot(values={'bar': []}, next=('__start__',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0c3cf0-4c0d-6791-bfff-8f817ecb842f'}}, metadata={'source': 'input', 'step': -1, 'parents': {}}, created_at='2025-11-17T16:03:55.387837+00:00', parent_config=None, tasks=(PregelTask(id='e1cf9dc2-cdcc-0092-c7d8-41fcf6ae9016', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'foo': 'foo', 'bar': ['z']}),), interrupts=()) \n",
      "--------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states = []\n",
    "for state in graph.get_state_history(config):\n",
    "    states.append(state)\n",
    "[print(state, \"\\n--------------------\") for state in states]   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec1f77c",
   "metadata": {},
   "source": [
    "This cell just reformats the list of states stored by the checkpointer to make it easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b7e8a07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: -1\n",
      "Checkpoint ID: 1f0c3cf0-4c0d-6791-bfff-8f817ecb842f\n",
      "Node: __start__\n",
      "values: {'bar': []}\n",
      "result: {'foo': 'foo', 'bar': ['z']}\n",
      "--------\n",
      "Step: 0\n",
      "Checkpoint ID: 1f0c3cf0-4c45-6a9f-8000-7f87773528fd\n",
      "Node: node_a\n",
      "values: {'foo': 'foo', 'bar': ['z']}\n",
      "result: {'foo': 'a', 'bar': ['a']}\n",
      "--------\n",
      "Step: 1\n",
      "Checkpoint ID: 1f0c3cf0-4c54-6b2b-8001-4b280c0c71a0\n",
      "Node: node_b\n",
      "values: {'foo': 'a', 'bar': ['z', 'a']}\n",
      "result: {'foo': 'b', 'bar': ['b']}\n",
      "--------\n",
      "Step: 2\n",
      "Checkpoint ID: 1f0c3cf0-4c59-6847-8002-8201850d27dc\n",
      "values: {'foo': 'b', 'bar': ['z', 'a', 'b']}\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "for state in reversed([state for state in graph.get_state_history(config)]):\n",
    "    print(\"Step:\", state.metadata['step'])\n",
    "    print(\"Checkpoint ID:\", state.config['configurable']['checkpoint_id'])\n",
    "    print(\"Node:\", state.tasks[0].name) if state.tasks else None\n",
    "    print(\"values:\", state.values)\n",
    "    print(\"result:\", state.tasks[0].result) if state.tasks else None\n",
    "    # print(state)\n",
    "    print(\"--------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0983b96d",
   "metadata": {},
   "source": [
    "You can see there is a checkpoint ID for each step.  You can play the graph from any step using the checkpoint id.  To restart the graph from a specific step add the checkpoint_id to the configuration and then pass 'None' for the state, when invoking the graph.  The graph will continue a new run from the step you've selected, with all the state as it was for the first run.\n",
    "\n",
    "This is very useful when developing and debugging, or for application where you want long running state over multiple runs.\n",
    "\n",
    "Note that the checkpoint id will be different for every run, so you will need to copy one of the checkpoint IDs from above into the config below.  For instance, if you take the Id for step 1, you will just see step 2 run, with the state pass to step 1 the same as it was in the previous run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2b9e5df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing Node A\n",
      "Adding: ['z'] + ['a']\n",
      "Executing Node B\n",
      "Adding: ['z', 'a'] + ['b']\n",
      "Final State: {'foo': 'b', 'bar': ['z', 'a', 'b']}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\" }, \"checkpoint_ns\": '',  \"checkpoint_id\": \"1f0c3cf0-4c45-6a9f-8000-7f87773528fd\"}\n",
    "result = graph.invoke(None, config)\n",
    "print(\"Final State:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec925dcb",
   "metadata": {},
   "source": [
    "## MESSAGE MANAGEMENT\n",
    "For long running 'conversations', between LLM and tools (or if there are large messages) then the context window can start to full up, and make it increasingly difficult for the LLM to be able to correctly read all the content.\n",
    "\n",
    "In cases where it is necessary to have these long running conversations then there are techniques to reduce the size of the context window:\n",
    "- Triming\n",
    "- Summarization\n",
    "- Deletion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7276a5",
   "metadata": {},
   "source": [
    "### TRIMING MESSAGES\n",
    "Triming means removing messages from the messages.  You can define the criteria for removal, but typically you will want older message removed and want to ensure the sequence of Human message, AI message, tool message is maintained.  Optionally (but normally) you will want to retain the system message.  You can specify the max number of tokens and the function will try to maintain that, while ensure the message sequence is maintained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "9e9152be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim messages\n",
    "from langchain_core.messages.utils import (\n",
    "    trim_messages,  \n",
    "    count_tokens_approximately  \n",
    ")\n",
    "\n",
    "# import systemmessage, humanmessage, aimessage, toolmessage\n",
    "from langchain_core.messages import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage,\n",
    "    ToolMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca0dcef",
   "metadata": {},
   "source": [
    "## Sample Message History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "9ea37642",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  This is a dummy conversation with multiple messages to demonstrate trimming\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are an assistant expert in writing SQL. You can take SQL queries and optimize them for performance.\"),\n",
    "    HumanMessage(content=\"Optimize this SQL query: SELECT * FROM users WHERE age > 30 AND status = 'active';\"),\n",
    "    AIMessage(content=\"Optimize the SQL query to improve performance by adding appropriate indexes and avoiding SELECT *.\", additional_kwargs={}),\n",
    "    HumanMessage(content=\"Optimize this SQL query: SELECT name, email FROM customers WHERE country = 'US' AND signup_date > '2022-01-01';\"),\n",
    "    AIMessage(content=\"To optimize the SQL query, consider adding an index on the country and signup_date columns.\", additional_kwargs={}),\n",
    "    HumanMessage(content=\"Optimize this SQL query: SELECT order_id, total_amount FROM orders WHERE order_date BETWEEN '2023-01-01' AND '2023-12-31' AND status = 'completed';\"),\n",
    "    AIMessage(content=\"To optimize the SQL query, create a composite index on order_date and status columns.\", additional_kwargs={}),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8996b1ce",
   "metadata": {},
   "source": [
    "### trim_messsages class from LangChain\n",
    "Pass a messsages list to the utility to trim the list.\n",
    "\n",
    "The number of tokens to be kept can be specified, the utility keeps whole messages.  You can also specify if you want to keep the System message and decide which message type to start and end on, to keep the correct sequence of messages after trimming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "4cfea440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim utility provided by LangChain to limit messages based on token count\n",
    "trimmed_messages = trim_messages(\n",
    "    messages, \n",
    "    max_tokens=200,\n",
    "    token_counter=count_tokens_approximately,\n",
    "    strategy=\"last\",\n",
    "    # start_on=\"human\",\n",
    "    include_system=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "c6ead1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant expert in writing SQL. You can take SQL queries and optimize them for performance.\n",
      "Optimize this SQL query: SELECT order_id, total_amount FROM orders WHERE order_date BETWEEN '2023-01-01' AND '2023-12-31' AND status = 'completed';\n",
      "To optimize the SQL query, create a composite index on order_date and status columns.\n"
     ]
    }
   ],
   "source": [
    "for  m in trimmed_messages:\n",
    "    print(m.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "50d8f68c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Any, TypedDict\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.messages import AnyMessage, RemoveMessage\n",
    "from langgraph.types import Overwrite\n",
    "from langchain_core.messages.utils import count_tokens_approximately\n",
    "from langgraph.graph import StateGraph, START, MessagesState\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "f259868f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "class State(MessagesState):\n",
    "    summary: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "69138f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_conversation(state: State):\n",
    "\n",
    "    # First, we get any existing summary\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "\n",
    "    # Create our summarization prompt\n",
    "    if summary:\n",
    "\n",
    "        # A summary already exists\n",
    "        summary_message = (\n",
    "            f\"This is a summary of the conversation to date: {summary}\\n\\n\"\n",
    "            \"Extend the summary by taking into account the new messages above.\"\n",
    "            \" Make sure you retain information about the user.\"\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        summary_message = \"Create a summary of the conversation above:\"\n",
    "\n",
    "    # Add prompt to our history\n",
    "    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n",
    "    response = model.invoke(messages)\n",
    "\n",
    "    # Delete all but the 2 most recent messages\n",
    "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
    "    return {\"summary\": response.content, \"messages\": delete_messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "10d25b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model(state: State):  \n",
    "    response = model.invoke( [HumanMessage(content=state['summary'])] + state[\"messages\"])\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "fc6afe3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: content=\"You've introduced yourself as **Bob**.\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'} id='lc_run--4ffec721-0273-495b-ad48-39cc4555a118-0' usage_metadata={'input_tokens': 135, 'output_tokens': 9, 'total_tokens': 144, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "checkpointer = InMemorySaver()\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(call_model)\n",
    "builder.add_node(\"summarize\", summarize_conversation)  \n",
    "builder.add_edge(START, \"summarize\")\n",
    "builder.add_edge(\"summarize\", \"call_model\")\n",
    "builder.add_edge(\"call_model\", END)\n",
    "graph = builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "# Invoke the graph\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "res1 = graph.invoke({\"messages\": \"hi, my name is bob\", \"summary\": \"\"}, config)\n",
    "res2 = graph.invoke({\"messages\": \"write a short poem about cats\", \"summary\": res1[\"summary\"]}, config)\n",
    "res3 = graph.invoke({\"messages\": \"now do the same but for dogs\", \"summary\": res2[\"summary\"]}, config)\n",
    "final_response = graph.invoke({\"messages\": \"what's my name?\", \"summary\": res3[\"summary\"]}, config)\n",
    "\n",
    "print(\"Final Response:\", final_response[\"messages\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2adef281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [AIMessage(content=\"Okay, Bob! Here's a poem about dogs, inspired by your prompt:\\n\\nA happy bark, a wagging tail,\\nLoyalty's promise, that will not fail.\\nA wet nose nudges, a joyful bound,\\nThe best of friends, that can be found.\\nWith ears that perk and eyes so bright,\\nThey fill our days with pure delight.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--9619bac3-20d7-4140-b8ed-3b0bbaead998-0', usage_metadata={'input_tokens': 145, 'output_tokens': 80, 'total_tokens': 225, 'input_token_details': {'cache_read': 0}}),\n",
       "  HumanMessage(content=\"what's my name?\", additional_kwargs={}, response_metadata={}, id='dd4125d2-a2c9-4cc4-a7f6-50c6d8c468d1'),\n",
       "  AIMessage(content=\"You've been calling me Bob, so I've been addressing you as Bob.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--602bd751-cdf0-4e2b-970a-b5637ec50ae8-0', usage_metadata={'input_tokens': 137, 'output_tokens': 18, 'total_tokens': 155, 'input_token_details': {'cache_read': 0}})],\n",
       " 'summary': 'This is a summary of the conversation to date:\\n\\nThe user asked for a poem about dogs. The AI responded with a poem about dogs, addressing the user as \"Bob.\" The user then asked for the AI to state their name.'}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3df2df5",
   "metadata": {},
   "source": [
    "## MULTIPLE STATE SCHEMAS\n",
    "We have been defining state schema with all the graphs.  We can add any property to our state schema and use it as a way to carry data between nodes (and conditional edges).  We can use this as a way to carry data between nodes and then select which attributes we will use in the state for the next node.\n",
    "\n",
    "Usually the same state schema (class definition) will pass through all nodes.  However, it is possible to define different input and output schemas for a graph, such that schema information may only exist in one part of the graph.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "178c623f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'graph_output': 'My name is Lance'}"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class InputState(TypedDict):\n",
    "    user_input: str\n",
    "\n",
    "class OutputState(TypedDict):\n",
    "    graph_output: str\n",
    "\n",
    "class OverallState(TypedDict):\n",
    "    foo: str\n",
    "    user_input: str\n",
    "    graph_output: str\n",
    "\n",
    "class PrivateState(TypedDict):\n",
    "    bar: str\n",
    "\n",
    "def node_1(state: InputState) -> OverallState:\n",
    "    # Write to OverallState\n",
    "    return {\"foo\": state[\"user_input\"] + \" name\"}\n",
    "\n",
    "def node_2(state: OverallState) -> PrivateState:\n",
    "    # Read from OverallState, write to PrivateState\n",
    "    return {\"bar\": state[\"foo\"] + \" is\"}\n",
    "\n",
    "def node_3(state: PrivateState) -> OutputState:\n",
    "    # Read from PrivateState, write to OutputState\n",
    "    return {\"graph_output\": state[\"bar\"] + \" Lance\"}\n",
    "\n",
    "builder = StateGraph(OverallState,input_schema=InputState,output_schema=OutputState)\n",
    "builder.add_node(\"node_1\", node_1)\n",
    "builder.add_node(\"node_2\", node_2)\n",
    "builder.add_node(\"node_3\", node_3)\n",
    "builder.add_edge(START, \"node_1\")\n",
    "builder.add_edge(\"node_1\", \"node_2\")\n",
    "builder.add_edge(\"node_2\", \"node_3\")\n",
    "builder.add_edge(\"node_3\", END)\n",
    "\n",
    "# add checkpointer\n",
    "checkpointer = InMemorySaver()\n",
    "graph = builder.compile(checkpointer=checkpointer)\n",
    "# create config\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "graph.invoke({\"user_input\":\"My\"}, config=config)\n",
    "# {'graph_output': 'My name is Lance'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "075efe68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: -1\n",
      "Checkpoint ID: 1f0c3390-0498-6b79-bfff-02aa23433c33\n",
      "Node: __start__\n",
      "values: {}\n",
      "result: {'user_input': 'My'}\n",
      "--------\n",
      "Step: 0\n",
      "Checkpoint ID: 1f0c3390-049a-65a8-8000-6310151bb7df\n",
      "Node: node_1\n",
      "values: {'user_input': 'My'}\n",
      "result: {'foo': 'My name'}\n",
      "--------\n",
      "Step: 1\n",
      "Checkpoint ID: 1f0c3390-049b-6941-8001-fa1722b2f9c1\n",
      "Node: node_2\n",
      "values: {'foo': 'My name', 'user_input': 'My'}\n",
      "result: {'bar': 'My name is'}\n",
      "--------\n",
      "Step: 2\n",
      "Checkpoint ID: 1f0c3390-049c-6b16-8002-bdfd6e1b1856\n",
      "Node: node_3\n",
      "values: {'foo': 'My name', 'user_input': 'My', 'bar': 'My name is'}\n",
      "result: {'graph_output': 'My name is Lance'}\n",
      "--------\n",
      "Step: 3\n",
      "Checkpoint ID: 1f0c3390-049d-6415-8003-4ff1b2f47d4b\n",
      "values: {'foo': 'My name', 'user_input': 'My', 'graph_output': 'My name is Lance', 'bar': 'My name is'}\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "for state in reversed([state for state in graph.get_state_history(config)]):\n",
    "    print(\"Step:\", state.metadata['step'])\n",
    "    print(\"Checkpoint ID:\", state.config['configurable']['checkpoint_id'])\n",
    "    print(\"Node:\", state.tasks[0].name) if state.tasks else None\n",
    "    print(\"values:\", state.values)\n",
    "    print(\"result:\", state.tasks[0].result) if state.tasks else None\n",
    "    # print(state)\n",
    "    print(\"--------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f19973",
   "metadata": {},
   "source": [
    "# SUBGRAPHS\n",
    "A subgraph enables one graph to call another graph.  Normally we consider the calling graph to be the parent and the invoked sub graph to be the child.  We can decompose this pattern as much as required.\n",
    "\n",
    "Subgraphs are a very power technique that allows us to create separate graphs for a particular task.  While they are performing that task they can maintain a local state required for the task.  The graph can return the result of the task back to the parent graph.  This makes it much easier to compartmentalize context to a particular task such that other parts of the graph do not need to share irrelevant context.\n",
    "\n",
    "Key to the interaction between parent and child graphs is how they share state.  There are two models for this:\n",
    "- Shared State\n",
    "- Separate State\n",
    "  \n",
    "In shared state, the parent graph state is passed to the child graph to populate with additional information.  Alternatively, a node can create and invoke the subgraph with a different state.  The node is then responsible for explicitly read and returning values from the parent state to the sub graph (note that a node can only call one subgraph)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6f07e9",
   "metadata": {},
   "source": [
    "## SubGraph with Separate State\n",
    "This simple example (without any LLM calls) shows how state is used between a parent and child subgraph.\n",
    "\n",
    "Note that the node calling the child graph needs to manage all state properties between the two graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "7cfc96dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((), {'node_1': {'foo': 'hi! foo'}})\n",
      "(('node_2:2f7a9e68-0588-2e64-7b01-43c5b44fe757',), {'subgraph_node_1': {'baz': 'baz'}})\n",
      "(('node_2:2f7a9e68-0588-2e64-7b01-43c5b44fe757',), {'subgraph_node_2': {'bar': 'hi! foobaz'}})\n",
      "((), {'node_2': {'foo': 'hi! foobaz'}})\n"
     ]
    }
   ],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.state import StateGraph, START\n",
    "\n",
    "# Define subgraph\n",
    "class SubgraphState(TypedDict):\n",
    "    # note that none of these keys are shared with the parent graph state\n",
    "    bar: str\n",
    "    baz: str\n",
    "\n",
    "def subgraph_node_1(state: SubgraphState):\n",
    "    return {\"baz\": \"baz\"}\n",
    "\n",
    "def subgraph_node_2(state: SubgraphState):\n",
    "    return {\"bar\": state[\"bar\"] + state[\"baz\"]}\n",
    "\n",
    "subgraph_builder = StateGraph(SubgraphState)\n",
    "subgraph_builder.add_node(subgraph_node_1)\n",
    "subgraph_builder.add_node(subgraph_node_2)\n",
    "subgraph_builder.add_edge(START, \"subgraph_node_1\")\n",
    "subgraph_builder.add_edge(\"subgraph_node_1\", \"subgraph_node_2\")\n",
    "subgraph = subgraph_builder.compile()\n",
    "\n",
    "# Define parent graph\n",
    "class ParentState(TypedDict):\n",
    "    foo: str\n",
    "\n",
    "def node_1(state: ParentState):\n",
    "    return {\"foo\": \"hi! \" + state[\"foo\"]}\n",
    "\n",
    "def node_2(state: ParentState):\n",
    "    # Transform the state to the subgraph state\n",
    "    response = subgraph.invoke({\"bar\": state[\"foo\"]})\n",
    "    # Transform response back to the parent state\n",
    "    return {\"foo\": response[\"bar\"]}\n",
    "\n",
    "\n",
    "builder = StateGraph(ParentState)\n",
    "builder.add_node(\"node_1\", node_1)\n",
    "builder.add_node(\"node_2\", node_2)\n",
    "builder.add_edge(START, \"node_1\")\n",
    "builder.add_edge(\"node_1\", \"node_2\")\n",
    "graph = builder.compile()\n",
    "\n",
    "for chunk in graph.stream({\"foo\": \"foo\"}, subgraphs=True):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5164fff",
   "metadata": {},
   "source": [
    "## Multi Level Subgraph with Separate States\n",
    "The pattern of parent and child graphs can be extended to any level of depth, allowing for further decomposition of flow and separations of concern and state with the agentic application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "d7b282e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((), {'parent_1': {'my_key': 'hi Bob'}})\n",
      "(('child:b9883265-8363-e281-b597-7f6827cddfa6', 'child_1:9c98878d-7413-12a1-42f6-3218cae37bcf'), {'grandchild_1': {'my_grandchild_key': 'hi Bob, how are you'}})\n",
      "(('child:b9883265-8363-e281-b597-7f6827cddfa6',), {'child_1': {'my_child_key': 'hi Bob, how are you today?'}})\n",
      "((), {'child': {'my_key': 'hi Bob, how are you today?'}})\n",
      "((), {'parent_2': {'my_key': 'hi Bob, how are you today? bye!'}})\n"
     ]
    }
   ],
   "source": [
    "# Grandchild graph\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.state import StateGraph, START, END\n",
    "\n",
    "class GrandChildState(TypedDict):\n",
    "    my_grandchild_key: str\n",
    "\n",
    "def grandchild_1(state: GrandChildState) -> GrandChildState:\n",
    "    # NOTE: child or parent keys will not be accessible here\n",
    "    return {\"my_grandchild_key\": state[\"my_grandchild_key\"] + \", how are you\"}\n",
    "\n",
    "\n",
    "grandchild = StateGraph(GrandChildState)\n",
    "grandchild.add_node(\"grandchild_1\", grandchild_1)\n",
    "\n",
    "grandchild.add_edge(START, \"grandchild_1\")\n",
    "grandchild.add_edge(\"grandchild_1\", END)\n",
    "\n",
    "grandchild_graph = grandchild.compile()\n",
    "\n",
    "# Child graph\n",
    "class ChildState(TypedDict):\n",
    "    my_child_key: str\n",
    "\n",
    "def call_grandchild_graph(state: ChildState) -> ChildState:\n",
    "    # NOTE: parent or grandchild keys won't be accessible here\n",
    "    grandchild_graph_input = {\"my_grandchild_key\": state[\"my_child_key\"]}\n",
    "    grandchild_graph_output = grandchild_graph.invoke(grandchild_graph_input)\n",
    "    return {\"my_child_key\": grandchild_graph_output[\"my_grandchild_key\"] + \" today?\"}\n",
    "\n",
    "child = StateGraph(ChildState)\n",
    "# We're passing a function here instead of just compiled graph (`grandchild_graph`)\n",
    "child.add_node(\"child_1\", call_grandchild_graph)\n",
    "child.add_edge(START, \"child_1\")\n",
    "child.add_edge(\"child_1\", END)\n",
    "child_graph = child.compile()\n",
    "\n",
    "# Parent graph\n",
    "class ParentState(TypedDict):\n",
    "    my_key: str\n",
    "\n",
    "def parent_1(state: ParentState) -> ParentState:\n",
    "    # NOTE: child or grandchild keys won't be accessible here\n",
    "    return {\"my_key\": \"hi \" + state[\"my_key\"]}\n",
    "\n",
    "def parent_2(state: ParentState) -> ParentState:\n",
    "    return {\"my_key\": state[\"my_key\"] + \" bye!\"}\n",
    "\n",
    "def call_child_graph(state: ParentState) -> ParentState:\n",
    "    child_graph_input = {\"my_child_key\": state[\"my_key\"]}\n",
    "    child_graph_output = child_graph.invoke(child_graph_input)\n",
    "    return {\"my_key\": child_graph_output[\"my_child_key\"]}\n",
    "\n",
    "parent = StateGraph(ParentState)\n",
    "parent.add_node(\"parent_1\", parent_1)\n",
    "# We're passing a function here instead of just a compiled graph (`child_graph`)\n",
    "parent.add_node(\"child\", call_child_graph)\n",
    "parent.add_node(\"parent_2\", parent_2)\n",
    "\n",
    "parent.add_edge(START, \"parent_1\")\n",
    "parent.add_edge(\"parent_1\", \"child\")\n",
    "parent.add_edge(\"child\", \"parent_2\")\n",
    "parent.add_edge(\"parent_2\", END)\n",
    "\n",
    "parent_graph = parent.compile()\n",
    "\n",
    "for chunk in parent_graph.stream({\"my_key\": \"Bob\"}, subgraphs=True):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e65c212",
   "metadata": {},
   "source": [
    "## Shared State SubGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "d0cc1c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'node_1': {'foo': 'hi! foo'}}\n",
      "{'node_2': {'foo': 'hi! foobar'}}\n"
     ]
    }
   ],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.state import StateGraph, START\n",
    "\n",
    "# Define subgraph\n",
    "class SubgraphState(TypedDict):\n",
    "    foo: str  # shared with parent graph state\n",
    "    bar: str  # private to SubgraphState\n",
    "\n",
    "def subgraph_node_1(state: SubgraphState):\n",
    "    return {\"bar\": \"bar\"}\n",
    "\n",
    "def subgraph_node_2(state: SubgraphState):\n",
    "    # note that this node is using a state key ('bar') that is only available in the subgraph\n",
    "    # and is sending update on the shared state key ('foo')\n",
    "    return {\"foo\": state[\"foo\"] + state[\"bar\"]}\n",
    "\n",
    "subgraph_builder = StateGraph(SubgraphState)\n",
    "subgraph_builder.add_node(subgraph_node_1)\n",
    "subgraph_builder.add_node(subgraph_node_2)\n",
    "subgraph_builder.add_edge(START, \"subgraph_node_1\")\n",
    "subgraph_builder.add_edge(\"subgraph_node_1\", \"subgraph_node_2\")\n",
    "subgraph = subgraph_builder.compile()\n",
    "\n",
    "# Define parent graph\n",
    "class ParentState(TypedDict):\n",
    "    foo: str\n",
    "\n",
    "def node_1(state: ParentState):\n",
    "    return {\"foo\": \"hi! \" + state[\"foo\"]}\n",
    "\n",
    "builder = StateGraph(ParentState)\n",
    "builder.add_node(\"node_1\", node_1)\n",
    "builder.add_node(\"node_2\", subgraph)\n",
    "builder.add_edge(START, \"node_1\")\n",
    "builder.add_edge(\"node_1\", \"node_2\")\n",
    "graph = builder.compile()\n",
    "\n",
    "for chunk in graph.stream({\"foo\": \"foo\"}):\n",
    "    print(chunk)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic-labs (3.13.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
