{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1912b53",
   "metadata": {},
   "source": [
    "# LangSmith - Example of Creating an Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54eb6f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "# create LangSmith client\n",
    "client = Client()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66619c7",
   "metadata": {},
   "source": [
    "## Create the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c8147f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset\n",
    "\n",
    "dataset_name = \"QA Example Dataset 1\"\n",
    "dataset = client.create_dataset(dataset_name)\n",
    "# if the dataset already exists, you can retrieve it like this:\n",
    "# dataset = client.list_datasets(dataset_name=dataset_name).__next__()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304142f1",
   "metadata": {},
   "source": [
    "## Populate the Dataset with Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43b2142d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'example_ids': ['d47d460e-fcd0-43ee-93a6-68be98fb0908',\n",
       "  '674f72c8-2c83-4ad0-ae57-070fa53a0ed1',\n",
       "  '841d296d-99c3-4b25-8831-774305a66889',\n",
       "  'e8ef35a2-4e1d-4b8e-b09a-e8575c417aff',\n",
       "  'd34b95ec-93f4-4f8f-bef8-acf7b19abfe3'],\n",
       " 'count': 5}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "client.create_examples(\n",
    "    dataset_id=dataset.id,\n",
    "    examples=[\n",
    "        {\n",
    "            \"inputs\": {\"question\": \"What is LangChain?\"},\n",
    "            \"outputs\": {\"answer\": \"A framework for building LLM applications\"},\n",
    "        },\n",
    "        {\n",
    "            \"inputs\": {\"question\": \"What is LangSmith?\"},\n",
    "            \"outputs\": {\"answer\": \"A platform for observing and evaluating LLM applications\"},\n",
    "        },\n",
    "        {\n",
    "            \"inputs\": {\"question\": \"What is OpenAI?\"},\n",
    "            \"outputs\": {\"answer\": \"A company that creates Large Language Models\"},\n",
    "        },\n",
    "        {\n",
    "            \"inputs\": {\"question\": \"What is Google?\"},\n",
    "            \"outputs\": {\"answer\": \"A technology company known for search\"},\n",
    "        },\n",
    "        {\n",
    "            \"inputs\": {\"question\": \"What is Mistral?\"},\n",
    "            \"outputs\": {\"answer\": \"A company that creates Large Language Models\"},\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910765ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a split from the dataset\n",
    "examples=[example.id for example in dataset.list_examples()]\n",
    "split = client.crea  create_split(\n",
    "    dataset_id=dataset.id,\n",
    "    name=\"evaluation_split\",\n",
    "    example_ids=examples[3:]  # use last two examples for evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784261f8",
   "metadata": {},
   "source": [
    "## Create the Evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0187277f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import openai\n",
    "from langsmith import wrappers\n",
    "import os\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79f5c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6ae68d",
   "metadata": {},
   "source": [
    "### Create the Concision Evaluator\n",
    "This simply compares the number of words produced w.r.t to the reference output, and returns as a score (less is better).\n",
    "This could also be a simple boolean test to check word count is below a certain level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dca2dc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concision(outputs: dict, reference_outputs: dict) -> int:\n",
    "    \"\"\"Evaluate the conciseness of the response compared to the reference answer.\n",
    "    Returns a positive score if the response is shorter than the reference answer,\"\"\"\n",
    "    # count words in outputs['response'] and reference_outputs['answer']\n",
    "    resp_wc = outputs['response'].split()\n",
    "    ref_wc = reference_outputs['answer'].split()\n",
    "\n",
    "    print(f\"Response length: {len(resp_wc)}, Reference length: {len(ref_wc)}\")\n",
    "    return len(ref_wc) - len(resp_wc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20b433f",
   "metadata": {},
   "source": [
    "### Correctness Evaluator\n",
    "This used an LLM to assess whether the output is correct.  It compares it to the reference answer in order to grade how correct the output is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "52cc8a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use SDK to wrap Gemini LLM\n",
    "model = openai.Client( api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
    "                        base_url=os.getenv(\"GEMINI_API_BASE\"))\n",
    "llm = wrappers.wrap_openai(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d352ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt to evaluate correctness\n",
    "user_prompt = \"\"\"\"\n",
    "You are grading the following question:\n",
    "{question}\n",
    "Here is the correct answer:\n",
    "{ref_answer}\n",
    "You are grading the following predicted answer:\n",
    "{answer}\n",
    "Provide a score from 1 to 5, where 5 is completely correct and 1 is completely incorrect:\n",
    "Score:\n",
    "Provide a brief explanation of the score:\n",
    "Comment:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "eval_prompt = \" You are an expert professor in grading student answers to questions.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "317ebf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a structured output to ensure LLM returns a score.  Also returns a comment to provide explanation of score.\n",
    "from pydantic import BaseModel, Field\n",
    "class CorrectnessEvalSchema(BaseModel):\n",
    "    \"\"\"CLass to define the schema for correctness evaluation.\"\"\"\n",
    "    score: int = Field(description=\"An integer score from 1 to 5 indicating the correctness of the answer\")\n",
    "    comment: str = Field(description=\"A brief explanation of the score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0228ff6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def q_correctness(inputs: dict,\n",
    "                outputs: dict,\n",
    "                reference_outputs: dict) -> int:\n",
    "    \"\"\" Evaluate the correctness of the response using Gemini LLM with a structured output.\"\"\"\n",
    "    # extract response from outputs\n",
    "    resp =  outputs['response']\n",
    "    # call Gemini LLM with evaluation prompt\n",
    "    response = llm.chat.completions.parse(\n",
    "        model=\"gemini-2.5-flash-lite\",\n",
    "        messages = [{\"role\": \"system\", \"content\": eval_prompt},\n",
    "                    {\"role\": \"user\", \n",
    "                     \"content\": user_prompt.format(question=inputs['question'],\n",
    "                                                   answer=resp,\n",
    "                                                   ref_answer=reference_outputs['answer'])}],\n",
    "                temperature = 0,\n",
    "                response_format=CorrectnessEvalSchema,\n",
    "    )\n",
    "    # extract score and comment from response\n",
    "    result = response.choices[0].message.parsed\n",
    "    print(f\"Score: {result.score}, Comment: {result.comment}\")\n",
    "    # return score and comment as a dict. LangSmith expects a dict return type with these keys.\n",
    "    # You can also return just an integer score or boolean if you prefer.\n",
    "    return {\"score\": result.score, \"comment\": result.comment}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e837200",
   "metadata": {},
   "source": [
    "### Create the Example Application\n",
    "This is a simple call to an LLM to get the answer to the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f2d33a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_app(question: str ):\n",
    "    instruction = \"Response to the question in a very brief, concise manner (one short sentence)\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": instruction},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    resp = llm.chat.completions.create(\n",
    "            model = \"gemini-2.5-flash-lite\",\n",
    "            messages=messages,\n",
    "            temperature=0.0,)\n",
    "            \n",
    "    return resp.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a762ed",
   "metadata": {},
   "source": [
    "### Wrap the Application\n",
    "This function calls the application and returns the response as a dictionary.  This response is used in the output.  You can include multiple keys in the output to use in your evaluators, if required.  You can wrap any function here, for instance if you evaluating a graph, you can invoke the graph inside this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eb420360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ls_target(inputs: dict) -> dict:\n",
    "    return {\"response\": my_app(inputs['question'])}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5fd9f8",
   "metadata": {},
   "source": [
    "### Execute the Evaluation\n",
    "Finally execute the evaluation.  Pass the function to call the target application, the dataset , the evaluators, and optionally provide an experiment prefix, to make the results easier to identify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217afe38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'exp-1869a2a9' at:\n",
      "https://eu.smith.langchain.com/o/3a7d9ffc-8b3e-42a7-9129-2ca245324f40/datasets/3ff3f321-1ea3-462a-8bd0-1461ecc7d5da/compare?selectedSessions=330b328d-118b-4a46-972a-b1fba9cf9326\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response length: 11, Reference length: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:21, 21.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 4, Comment: The answer is very close to the correct answer. It correctly identifies LangSmith as a platform for LLM applications and includes key functionalities like developing, testing, and monitoring, which are all part of observing and evaluating. The slight difference in wording ('observing and evaluating' vs. 'developing, testing, and monitoring') prevents a perfect score, but it's a strong answer.\n",
      "Response length: 7, Reference length: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:22,  9.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 4, Comment: The answer is mostly correct, as OpenAI is indeed an AI research laboratory. However, it could be more specific by mentioning their work with Large Language Models, which is a key aspect of their identity and output.\n",
      "Response length: 10, Reference length: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:24,  5.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 4, Comment: The answer correctly identifies Mistral as a large language model and mentions the developing company. However, the question asks 'What is Mistral?', implying the entity itself, not just its product. The correct answer focuses on Mistral being a company that creates LLMs, which is a more direct answer to the question.\n",
      "Response length: 11, Reference length: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:25,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 5, Comment: The predicted answer is a perfect match for the correct answer, accurately defining LangChain as a framework for developing applications powered by language models.\n",
      "Response length: 12, Reference length: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:26,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 5, Comment: The predicted answer is comprehensive and accurate, correctly identifying Google as a multinational technology company specializing in internet-related services and products. This aligns perfectly with the expected answer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:26,  5.31s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.question</th>\n",
       "      <th>outputs.response</th>\n",
       "      <th>error</th>\n",
       "      <th>reference.answer</th>\n",
       "      <th>feedback.concision</th>\n",
       "      <th>feedback.q_correctness</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>example_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is LangSmith?</td>\n",
       "      <td>LangSmith is a platform for developing, testin...</td>\n",
       "      <td>None</td>\n",
       "      <td>A platform for observing and evaluating LLM ap...</td>\n",
       "      <td>-3</td>\n",
       "      <td>4</td>\n",
       "      <td>20.535326</td>\n",
       "      <td>674f72c8-2c83-4ad0-ae57-070fa53a0ed1</td>\n",
       "      <td>df97ac9f-6120-4073-852f-04dc5d484148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is OpenAI?</td>\n",
       "      <td>OpenAI is an artificial intelligence research ...</td>\n",
       "      <td>None</td>\n",
       "      <td>A company that creates Large Language Models</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.598477</td>\n",
       "      <td>841d296d-99c3-4b25-8831-774305a66889</td>\n",
       "      <td>e3f2967c-6e3b-4ddf-9a5b-2c64de886ef6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is Mistral?</td>\n",
       "      <td>Mistral is a large language model developed by...</td>\n",
       "      <td>None</td>\n",
       "      <td>A company that creates Large Language Models</td>\n",
       "      <td>-3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.564737</td>\n",
       "      <td>d34b95ec-93f4-4f8f-bef8-acf7b19abfe3</td>\n",
       "      <td>1a64c81b-f5b3-4445-9dc9-de91320c20fd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is LangChain?</td>\n",
       "      <td>LangChain is a framework for developing applic...</td>\n",
       "      <td>None</td>\n",
       "      <td>A framework for building LLM applications</td>\n",
       "      <td>-5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.307963</td>\n",
       "      <td>d47d460e-fcd0-43ee-93a6-68be98fb0908</td>\n",
       "      <td>9e174744-48ae-411d-b24c-4b519384a8af</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is Google?</td>\n",
       "      <td>Google is a multinational technology company s...</td>\n",
       "      <td>None</td>\n",
       "      <td>A technology company known for search</td>\n",
       "      <td>-6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.368799</td>\n",
       "      <td>e8ef35a2-4e1d-4b8e-b09a-e8575c417aff</td>\n",
       "      <td>4dd41f7f-3e9e-48ae-9c63-85a110b6069b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<ExperimentResults exp-1869a2a9>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This call will evaluate the examples in the split using the two evaluators defined above.\n",
    "# It will use the whole dataset.\n",
    "client.evaluate(\n",
    "    ls_target,\n",
    "    dataset_name,\n",
    "    evaluators=[concision, q_correctness],\n",
    "    experiment_prefix=\"exp\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3a5657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF you create a split, you can evaluate just that split like this.eval\n",
    "# In this case the split is called \"test1\", but you can replace with your split name.\n",
    "# You need to create the split first in LangSmith.\n",
    "results = client.evaluate(\n",
    "    ls_target,\n",
    "    data=client.list_examples(dataset_name=dataset_name, splits=[\"test1\"]),\n",
    "    evaluators=[concision, q_correctness],\n",
    "    experiment_prefix=\"exp\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic-labs (3.13.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
