{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1912b53",
   "metadata": {},
   "source": [
    "# LangSmith - Example of Creating an Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54eb6f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "# create LangSmith client\n",
    "client = Client()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66619c7",
   "metadata": {},
   "source": [
    "## Create the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14c8147f",
   "metadata": {},
   "outputs": [
    {
     "ename": "LangSmithConflictError",
     "evalue": "Conflict for /datasets. HTTPError('409 Client Error: Conflict for url: https://api.smith.langchain.com/datasets', '{\"detail\":\"Dataset with this name already exists.\"}')",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Foundations_of_AgenticAI/agentic-labs/.venv/lib/python3.12/site-packages/langsmith/utils.py:159\u001b[39m, in \u001b[36mraise_for_status_with_text\u001b[39m\u001b[34m(response)\u001b[39m\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m requests.HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Foundations_of_AgenticAI/agentic-labs/.venv/lib/python3.12/site-packages/requests/models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 409 Client Error: Conflict for url: https://api.smith.langchain.com/datasets",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Foundations_of_AgenticAI/agentic-labs/.venv/lib/python3.12/site-packages/langsmith/client.py:950\u001b[39m, in \u001b[36mClient.request_with_retries\u001b[39m\u001b[34m(self, method, pathname, request_kwargs, stop_after_attempt, retry_on, to_ignore, handle_response, _context, **kwargs)\u001b[39m\n\u001b[32m    944\u001b[39m     response = \u001b[38;5;28mself\u001b[39m.session.request(\n\u001b[32m    945\u001b[39m         method,\n\u001b[32m    946\u001b[39m         _construct_url(\u001b[38;5;28mself\u001b[39m.api_url, pathname),\n\u001b[32m    947\u001b[39m         stream=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    948\u001b[39m         **request_kwargs,\n\u001b[32m    949\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m950\u001b[39m \u001b[43mls_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status_with_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Foundations_of_AgenticAI/agentic-labs/.venv/lib/python3.12/site-packages/langsmith/utils.py:161\u001b[39m, in \u001b[36mraise_for_status_with_text\u001b[39m\u001b[34m(response)\u001b[39m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m requests.HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m requests.HTTPError(\u001b[38;5;28mstr\u001b[39m(e), response.text) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mHTTPError\u001b[39m: [Errno 409 Client Error: Conflict for url: https://api.smith.langchain.com/datasets] {\"detail\":\"Dataset with this name already exists.\"}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mLangSmithConflictError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# create a dataset\u001b[39;00m\n\u001b[32m      3\u001b[39m dataset_name = \u001b[33m\"\u001b[39m\u001b[33mQA Example Dataset 1\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m dataset = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# if the dataset already exists, you can retrieve it like this:\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# dataset = client.list_datasets(dataset_name=dataset_name).__next__()\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Foundations_of_AgenticAI/agentic-labs/.venv/lib/python3.12/site-packages/langsmith/client.py:3828\u001b[39m, in \u001b[36mClient.create_dataset\u001b[39m\u001b[34m(self, dataset_name, description, data_type, inputs_schema, outputs_schema, transformations, metadata)\u001b[39m\n\u001b[32m   3825\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m outputs_schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3826\u001b[39m     dataset[\u001b[33m\"\u001b[39m\u001b[33moutputs_schema_definition\u001b[39m\u001b[33m\"\u001b[39m] = outputs_schema\n\u001b[32m-> \u001b[39m\u001b[32m3828\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest_with_retries\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3829\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3830\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/datasets\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3831\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mContent-Type\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mapplication/json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3832\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_orjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3833\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3834\u001b[39m ls_utils.raise_for_status_with_text(response)\n\u001b[32m   3836\u001b[39m json_response = response.json()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Foundations_of_AgenticAI/agentic-labs/.venv/lib/python3.12/site-packages/langsmith/client.py:995\u001b[39m, in \u001b[36mClient.request_with_retries\u001b[39m\u001b[34m(self, method, pathname, request_kwargs, stop_after_attempt, retry_on, to_ignore, handle_response, _context, **kwargs)\u001b[39m\n\u001b[32m    990\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ls_utils.LangSmithNotFoundError(\n\u001b[32m    991\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mResource not found for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpathname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_context\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    993\u001b[39m     )\n\u001b[32m    994\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m response.status_code == \u001b[32m409\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m995\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ls_utils.LangSmithConflictError(\n\u001b[32m    996\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mConflict for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpathname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m_context\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    997\u001b[39m     )\n\u001b[32m    998\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m response.status_code == \u001b[32m403\u001b[39m:\n\u001b[32m    999\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mLangSmithConflictError\u001b[39m: Conflict for /datasets. HTTPError('409 Client Error: Conflict for url: https://api.smith.langchain.com/datasets', '{\"detail\":\"Dataset with this name already exists.\"}')"
     ]
    }
   ],
   "source": [
    "# create a dataset\n",
    "\n",
    "dataset_name = \"QA Example Dataset 1\"\n",
    "dataset = client.create_dataset(dataset_name)\n",
    "# if the dataset already exists, you can retrieve it like this:\n",
    "# dataset = client.list_datasets(dataset_name=dataset_name).__next__()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304142f1",
   "metadata": {},
   "source": [
    "## Populate the Dataset with Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43b2142d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'example_ids': ['65f271ca-333d-420d-8e93-01d1e0e4da75',\n",
       "  'd1566b24-8470-42f2-a5a8-3db096ca702a',\n",
       "  'e076a36d-ca17-49e8-b75c-7dadb711bd75',\n",
       "  '4dadfed7-129d-4f50-9ddd-00a4b57d8b95',\n",
       "  'db23c766-5f72-4907-ae14-9a33b1319619'],\n",
       " 'count': 5}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "client.create_examples(\n",
    "    dataset_id=dataset.id,\n",
    "    examples=[\n",
    "        {\n",
    "            \"inputs\": {\"question\": \"What is LangChain?\"},\n",
    "            \"outputs\": {\"answer\": \"A framework for building LLM applications\"},\n",
    "        },\n",
    "        {\n",
    "            \"inputs\": {\"question\": \"What is LangSmith?\"},\n",
    "            \"outputs\": {\"answer\": \"A platform for observing and evaluating LLM applications\"},\n",
    "        },\n",
    "        {\n",
    "            \"inputs\": {\"question\": \"What is OpenAI?\"},\n",
    "            \"outputs\": {\"answer\": \"A company that creates Large Language Models\"},\n",
    "        },\n",
    "        {\n",
    "            \"inputs\": {\"question\": \"What is Google?\"},\n",
    "            \"outputs\": {\"answer\": \"A technology company known for search\"},\n",
    "        },\n",
    "        {\n",
    "            \"inputs\": {\"question\": \"What is Mistral?\"},\n",
    "            \"outputs\": {\"answer\": \"A company that creates Large Language Models\"},\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "910765ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Client' object has no attribute 'create_split'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# create a split from the dataset\u001b[39;00m\n\u001b[32m      2\u001b[39m examples = [example.id \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m client.list_examples(dataset_id=dataset.id)]\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m split = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_split\u001b[49m(\n\u001b[32m      4\u001b[39m     dataset_id=dataset.id,\n\u001b[32m      5\u001b[39m     name=\u001b[33m\"\u001b[39m\u001b[33mevaluation_split\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     example_ids=examples[\u001b[32m3\u001b[39m:]  \u001b[38;5;66;03m# use last two examples for evaluation\u001b[39;00m\n\u001b[32m      7\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'Client' object has no attribute 'create_split'"
     ]
    }
   ],
   "source": [
    "# create a split from the dataset\n",
    "examples = [example.id for example in client.list_examples(dataset_id=dataset.id)]\n",
    "split = client.create_split(\n",
    "    dataset_id=dataset.id,\n",
    "    name=\"evaluation_split\",\n",
    "    example_ids=examples[3:]  # use last two examples for evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784261f8",
   "metadata": {},
   "source": [
    "## Create the Evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0187277f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import openai\n",
    "from langsmith import wrappers\n",
    "import os\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f79f5c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6ae68d",
   "metadata": {},
   "source": [
    "### Create the Concision Evaluator\n",
    "This simply compares the number of words produced w.r.t to the reference output, and returns as a score (less is better).\n",
    "This could also be a simple boolean test to check word count is below a certain level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dca2dc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concision(outputs: dict, reference_outputs: dict) -> int:\n",
    "    \"\"\"Evaluate the conciseness of the response compared to the reference answer.\n",
    "    Returns a positive score if the response is shorter than the reference answer,\"\"\"\n",
    "    # count words in outputs['response'] and reference_outputs['answer']\n",
    "    resp_wc = outputs['response'].split()\n",
    "    ref_wc = reference_outputs['answer'].split()\n",
    "\n",
    "    print(f\"Response length: {len(resp_wc)}, Reference length: {len(ref_wc)}\")\n",
    "    return len(ref_wc) - len(resp_wc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20b433f",
   "metadata": {},
   "source": [
    "### Correctness Evaluator\n",
    "This used an LLM to assess whether the output is correct.  It compares it to the reference answer in order to grade how correct the output is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52cc8a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use SDK to wrap Gemini LLM\n",
    "model = openai.Client( api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
    "                        base_url=os.getenv(\"GEMINI_API_BASE\"))\n",
    "llm = wrappers.wrap_openai(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19d352ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt to evaluate correctness\n",
    "user_prompt = \"\"\"\"\n",
    "You are grading the following question:\n",
    "{question}\n",
    "Here is the correct answer:\n",
    "{ref_answer}\n",
    "You are grading the following predicted answer:\n",
    "{answer}\n",
    "Provide a score from 1 to 5, where 5 is completely correct and 1 is completely incorrect:\n",
    "Score:\n",
    "Provide a brief explanation of the score:\n",
    "Comment:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "eval_prompt = \" You are an expert professor in grading student answers to questions.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "317ebf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a structured output to ensure LLM returns a score.  Also returns a comment to provide explanation of score.\n",
    "from pydantic import BaseModel, Field\n",
    "class CorrectnessEvalSchema(BaseModel):\n",
    "    \"\"\"CLass to define the schema for correctness evaluation.\"\"\"\n",
    "    score: int = Field(description=\"An integer score from 1 to 5 indicating the correctness of the answer\")\n",
    "    comment: str = Field(description=\"A brief explanation of the score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0228ff6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def q_correctness(inputs: dict,\n",
    "                outputs: dict,\n",
    "                reference_outputs: dict) -> int:\n",
    "    \"\"\" Evaluate the correctness of the response using Gemini LLM with a structured output.\"\"\"\n",
    "    # extract response from outputs\n",
    "    resp =  outputs['response']\n",
    "    # call Gemini LLM with evaluation prompt\n",
    "    response = llm.chat.completions.parse(\n",
    "        model=\"gemini-2.5-flash-lite\",\n",
    "        messages = [{\"role\": \"system\", \"content\": eval_prompt},\n",
    "                    {\"role\": \"user\", \n",
    "                     \"content\": user_prompt.format(question=inputs['question'],\n",
    "                                                   answer=resp,\n",
    "                                                   ref_answer=reference_outputs['answer'])}],\n",
    "                temperature = 0,\n",
    "                response_format=CorrectnessEvalSchema,\n",
    "    )\n",
    "    # extract score and comment from response\n",
    "    result = response.choices[0].message.parsed\n",
    "    print(f\"Score: {result.score}, Comment: {result.comment}\")\n",
    "    # return score and comment as a dict. LangSmith expects a dict return type with these keys.\n",
    "    # You can also return just an integer score or boolean if you prefer.\n",
    "    return {\"score\": result.score, \"comment\": result.comment}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e837200",
   "metadata": {},
   "source": [
    "### Create the Example Application\n",
    "This is a simple call to an LLM to get the answer to the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f2d33a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_app(question: str ):\n",
    "    instruction = \"Response to the question in a very brief, concise manner (one short sentence)\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": instruction},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    resp = llm.chat.completions.create(\n",
    "            model = \"gemini-2.5-flash-lite\",\n",
    "            messages=messages,\n",
    "            temperature=0.0,)\n",
    "            \n",
    "    return resp.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a762ed",
   "metadata": {},
   "source": [
    "### Wrap the Application\n",
    "This function calls the application and returns the response as a dictionary.  This response is used in the output.  You can include multiple keys in the output to use in your evaluators, if required.  You can wrap any function here, for instance if you evaluating a graph, you can invoke the graph inside this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb420360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ls_target(inputs: dict) -> dict:\n",
    "    return {\"response\": my_app(inputs['question'])}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5fd9f8",
   "metadata": {},
   "source": [
    "### Execute the Evaluation\n",
    "Finally execute the evaluation.  Pass the function to call the target application, the dataset , the evaluators, and optionally provide an experiment prefix, to make the results easier to identify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "217afe38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aidodo/Foundations_of_AgenticAI/agentic-labs/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'exp-802170a7' at:\n",
      "https://smith.langchain.com/o/964e7404-67a2-4b32-87f1-9ca489d8bbd5/datasets/f2b609c2-4f5c-4b98-ad25-7223e7770299/compare?selectedSessions=3e0b7ecd-04c4-4012-801d-789a9244bc6b\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response length: 11, Reference length: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 5, Comment: The predicted answer is a perfect match for the correct answer, accurately defining LangChain as a framework for developing applications powered by language models.\n",
      "Response length: 7, Reference length: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:03,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 4, Comment: The answer is mostly correct. OpenAI is indeed an AI research laboratory, and a significant part of its work involves creating Large Language Models. However, it could be more specific by mentioning the creation of LLMs as stated in the correct answer.\n",
      "Response length: 12, Reference length: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:04,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 5, Comment: The predicted answer is a comprehensive and accurate description of Google, aligning perfectly with the expected answer.\n",
      "Response length: 11, Reference length: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:05,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 4, Comment: The answer is very close to the correct answer. It correctly identifies LangSmith as a platform for LLM applications and includes key functionalities like developing, testing, and monitoring, which are all part of observing and evaluating. The slight difference in wording ('observing and evaluating' vs. 'developing, testing, and monitoring') prevents a perfect score, but it's a strong answer.\n",
      "Response length: 10, Reference length: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:07,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 4, Comment: The answer correctly identifies Mistral as a large language model and mentions the developing company. However, the question asks 'What is Mistral?', implying the entity itself, not just its product. While closely related, the ideal answer would focus on Mistral AI as a company.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:08,  1.60s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.question</th>\n",
       "      <th>outputs.response</th>\n",
       "      <th>error</th>\n",
       "      <th>reference.answer</th>\n",
       "      <th>feedback.concision</th>\n",
       "      <th>feedback.q_correctness</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>example_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is LangChain?</td>\n",
       "      <td>LangChain is a framework for developing applic...</td>\n",
       "      <td>None</td>\n",
       "      <td>A framework for building LLM applications</td>\n",
       "      <td>-5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.714097</td>\n",
       "      <td>4dadfed7-129d-4f50-9ddd-00a4b57d8b95</td>\n",
       "      <td>890d95c2-83ee-47f1-ba95-b7c89ac69699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is OpenAI?</td>\n",
       "      <td>OpenAI is an artificial intelligence research ...</td>\n",
       "      <td>None</td>\n",
       "      <td>A company that creates Large Language Models</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.660563</td>\n",
       "      <td>65f271ca-333d-420d-8e93-01d1e0e4da75</td>\n",
       "      <td>e8733198-c355-46e6-b9a8-d3c803d02d75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is Google?</td>\n",
       "      <td>Google is a multinational technology company s...</td>\n",
       "      <td>None</td>\n",
       "      <td>A technology company known for search</td>\n",
       "      <td>-6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.628674</td>\n",
       "      <td>d1566b24-8470-42f2-a5a8-3db096ca702a</td>\n",
       "      <td>480d06be-68ae-49df-97ab-dbdb7bac4e38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is LangSmith?</td>\n",
       "      <td>LangSmith is a platform for developing, testin...</td>\n",
       "      <td>None</td>\n",
       "      <td>A platform for observing and evaluating LLM ap...</td>\n",
       "      <td>-3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.609095</td>\n",
       "      <td>db23c766-5f72-4907-ae14-9a33b1319619</td>\n",
       "      <td>fba5e928-a64c-4c19-bd78-b54ebba91223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is Mistral?</td>\n",
       "      <td>Mistral is a large language model developed by...</td>\n",
       "      <td>None</td>\n",
       "      <td>A company that creates Large Language Models</td>\n",
       "      <td>-3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.588265</td>\n",
       "      <td>e076a36d-ca17-49e8-b75c-7dadb711bd75</td>\n",
       "      <td>59e082fe-11ea-4c05-9816-6f714c944ff3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<ExperimentResults exp-802170a7>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This call will evaluate the examples in the split using the two evaluators defined above.\n",
    "# It will use the whole dataset.\n",
    "client.evaluate(\n",
    "    ls_target,\n",
    "    dataset_name,\n",
    "    evaluators=[concision, q_correctness],\n",
    "    experiment_prefix=\"exp\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2f3a5657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'exp-b0ffc5dc' at:\n",
      "https://smith.langchain.com/o/964e7404-67a2-4b32-87f1-9ca489d8bbd5/datasets/f2b609c2-4f5c-4b98-ad25-7223e7770299/compare?selectedSessions=0c4bc322-b0f5-4f9a-9512-e22948bebdfe\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response length: 11, Reference length: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:02,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 4, Comment: The answer is very close to the correct answer. It correctly identifies LangSmith as a platform for LLM applications and includes key functionalities like developing, testing, and monitoring, which are all part of observing and evaluating. The slight difference in wording ('observing and evaluating' vs. 'developing, testing, and monitoring') prevents a perfect score, but it's a strong answer.\n",
      "Response length: 10, Reference length: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:03,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 4, Comment: The answer correctly identifies Mistral as a large language model and mentions the developing company. However, the question asks 'What is Mistral?', implying the entity itself, not just its product. While closely related, the ideal answer would focus on Mistral AI as a company.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:04,  2.18s/it]\n"
     ]
    }
   ],
   "source": [
    "# IF you create a split, you can evaluate just that split like this.eval\n",
    "# In this case the split is called \"test1\", but you can replace with your split name.\n",
    "# You need to create the split first in LangSmith.\n",
    "results = client.evaluate(\n",
    "    ls_target,\n",
    "    data=client.list_examples(dataset_name=dataset_name, splits=[\"test1\"]),\n",
    "    evaluators=[concision, q_correctness],\n",
    "    experiment_prefix=\"exp\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5bdc2085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available methods: ['add_runs_to_annotation_queue', 'aevaluate', 'aevaluate_run', 'api_key', 'api_url', 'arun_on_dataset', 'batch_ingest_runs', 'cleanup', 'clone_public_dataset', 'compressed_traces', 'create_annotation_queue', 'create_chat_example', 'create_commit', 'create_comparative_experiment', 'create_dataset', 'create_example', 'create_example_from_run', 'create_examples', 'create_feedback', 'create_feedback_from_token', 'create_llm_example', 'create_presigned_feedback_token', 'create_presigned_feedback_tokens', 'create_project', 'create_prompt', 'create_run', 'delete_annotation_queue', 'delete_dataset', 'delete_example', 'delete_examples', 'delete_feedback', 'delete_project', 'delete_prompt', 'delete_run_from_annotation_queue', 'diff_dataset_versions', 'evaluate', 'evaluate_run', 'flush', 'flush_compressed_traces', 'get_experiment_results', 'get_prompt', 'get_run_from_annotation_queue', 'get_run_stats', 'get_run_url', 'get_test_results', 'has_dataset', 'has_project', 'index_dataset', 'info', 'like_prompt', 'list_annotation_queues', 'list_dataset_splits', 'list_dataset_versions', 'list_datasets', 'list_examples', 'list_feedback', 'list_presigned_feedback_tokens', 'list_projects', 'list_prompt_commits', 'list_prompts', 'list_runs', 'list_shared_examples', 'list_shared_projects', 'list_shared_runs', 'multipart_ingest', 'otel_exporter', 'pull_prompt', 'pull_prompt_commit', 'push_prompt', 'read_annotation_queue', 'read_dataset', 'read_dataset_openai_finetuning', 'read_dataset_shared_schema', 'read_dataset_version', 'read_example', 'read_feedback', 'read_project', 'read_run', 'read_run_shared_link', 'read_shared_dataset', 'read_shared_run', 'request_with_retries', 'retry_config', 'run_is_shared', 'run_on_dataset', 'session', 'share_dataset', 'share_run', 'similar_examples', 'sync_indexed_dataset', 'timeout_ms', 'tracing_queue', 'tracing_sample_rate', 'unlike_prompt', 'unshare_dataset', 'unshare_run', 'update_annotation_queue', 'update_dataset_splits', 'update_dataset_tag', 'update_example', 'update_examples', 'update_examples_multipart', 'update_feedback', 'update_project', 'update_prompt', 'update_run', 'upload_csv', 'upload_dataframe', 'upload_examples_multipart', 'upsert_examples_multipart', 'workspace_id']\n"
     ]
    }
   ],
   "source": [
    "print(\"Available methods:\", [method for method in dir(client) if not method.startswith('_')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8209d171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'example_ids': ['0cf0576f-efaa-4d47-8ffc-d28cd86d0148',\n",
       "  '043a29e4-2a44-427c-a384-35073a2e8d89',\n",
       "  '89c6c017-bec4-4521-b72d-49a9d2c85688',\n",
       "  '4a8db125-c32c-4431-81d3-a28a48fec388',\n",
       "  'fb0c0aeb-6304-4e4e-b46b-b8ebaabd8b0a'],\n",
       " 'count': 5}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataset\n",
    "dataset = client.create_dataset(\"QA Example Dataset 2\")\n",
    "\n",
    "# Create examples with split tags\n",
    "examples_with_splits = [\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is LangChain?\"},\n",
    "        \"outputs\": {\"answer\": \"A framework for building LLM applications\"},\n",
    "        \"split\": \"train\"\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is LangSmith?\"},\n",
    "        \"outputs\": {\"answer\": \"A platform for observing and evaluating LLM applications\"},\n",
    "        \"split\": \"train\"\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is OpenAI?\"},\n",
    "        \"outputs\": {\"answer\": \"A company that creates Large Language Models\"},\n",
    "        \"split\": \"train\"\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is Google?\"},\n",
    "        \"outputs\": {\"answer\": \"A technology company known for search\"},\n",
    "        \"split\": \"eval\"\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is Mistral?\"},\n",
    "        \"outputs\": {\"answer\": \"A company that creates Large Language Models\"},\n",
    "        \"split\": \"eval\"\n",
    "    }\n",
    "]\n",
    "\n",
    "client.create_examples(\n",
    "    dataset_id=dataset.id,\n",
    "    examples=examples_with_splits\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c37cf6fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(name='QA Example Dataset 2', description=None, data_type=<DataType.kv: 'kv'>, id=UUID('02f20b15-23a0-4544-ac0c-ce49517545c7'), created_at=datetime.datetime(2025, 11, 13, 10, 0, 26, 74235, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2025, 11, 13, 10, 0, 26, 74235, tzinfo=datetime.timezone.utc), example_count=0, session_count=0, last_session_start_time=None, inputs_schema=None, outputs_schema=None, transformations=None, metadata={'runtime': {'sdk': 'langsmith-py', 'sdk_version': '0.4.35', 'library': 'langsmith', 'platform': 'Linux-6.14.0-34-generic-x86_64-with-glibc2.39', 'runtime': 'python', 'py_implementation': 'CPython', 'runtime_version': '3.12.3', 'langchain_version': '0.3.27', 'langchain_core_version': '0.3.79'}})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b471ce95",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = client.read_dataset(dataset_id='f2b609c2-4f5c-4b98-ad25-7223e7770299')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ea14d24c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(name='QA Example Dataset 1', description=None, data_type=<DataType.kv: 'kv'>, id=UUID('f2b609c2-4f5c-4b98-ad25-7223e7770299'), created_at=datetime.datetime(2025, 11, 13, 9, 15, 50, 894871, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2025, 11, 13, 9, 15, 50, 894871, tzinfo=datetime.timezone.utc), example_count=5, session_count=1, last_session_start_time=datetime.datetime(2025, 11, 13, 9, 28, 44, 853255), inputs_schema=None, outputs_schema=None, transformations=None, metadata={'runtime': {'sdk': 'langsmith-py', 'library': 'langsmith', 'runtime': 'python', 'platform': 'Linux-6.14.0-34-generic-x86_64-with-glibc2.39', 'sdk_version': '0.4.35', 'runtime_version': '3.12.3', 'langchain_version': '0.3.27', 'py_implementation': 'CPython', 'langchain_core_version': '0.3.79'}})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5cb3a695",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = client.list_examples(dataset_id='f2b609c2-4f5c-4b98-ad25-7223e7770299')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "47772dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Client.list_examples at 0x76c17339fa10>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "74537d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_examples = [example.id for example in client.list_examples(dataset_id=dataset.id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2e6f277d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[UUID('4dadfed7-129d-4f50-9ddd-00a4b57d8b95'),\n",
       " UUID('65f271ca-333d-420d-8e93-01d1e0e4da75'),\n",
       " UUID('d1566b24-8470-42f2-a5a8-3db096ca702a'),\n",
       " UUID('db23c766-5f72-4907-ae14-9a33b1319619'),\n",
       " UUID('e076a36d-ca17-49e8-b75c-7dadb711bd75')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c101b189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[UUID('db23c766-5f72-4907-ae14-9a33b1319619'),\n",
       " UUID('e076a36d-ca17-49e8-b75c-7dadb711bd75')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_examples[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "733ec882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': '2 examples updated',\n",
       " 'example_ids': ['db23c766-5f72-4907-ae14-9a33b1319619',\n",
       "  'e076a36d-ca17-49e8-b75c-7dadb711bd75'],\n",
       " 'count': 2}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.update_examples(\n",
    "  example_ids=my_examples[3:],\n",
    "  metadata=[{\"foo\": \"baz\"}, {\"foo\": \"qux\"}],\n",
    "  splits=[\"test1\", \"test1\"] # Splits can be arrays or standalone strings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a333c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic-labs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
