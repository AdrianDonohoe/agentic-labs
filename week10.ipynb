{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a84f78b7",
   "metadata": {},
   "source": [
    "# Model Context Protocol\n",
    "This notebook provides an overview of MCP client.\n",
    "\n",
    "Server code runs in .py files.\n",
    "\n",
    "Langchain provide a wrapper library for MCP.  Install with: \n",
    "- uv add \"mcp[cli]\"\n",
    "\n",
    "The library creates a unified object that contains connections to mulitple MCP servers.\n",
    "\n",
    "Langchain also offer the ability to inject arguments into tool calls.  This allows the developer to remove arguments that the LLM does not have to (or should not) see.  The is an important feature for both context management and security.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "607ad438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d35437f",
   "metadata": {},
   "source": [
    "MCP servers to connect to can be listed in the argument for MultiServerMCPCLient construction.\n",
    "\n",
    "The example below connects to the MCP server in 'get-time-server.py'.  Start the server using uv run (or python) and leave it running on the command line.  The server runs in streamable_http transport mode.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3fe00e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"math\": {\n",
    "            \"transport\": \"streamable_http\",\n",
    "            \"url\": \"http://localhost:8000/mcp\",\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e203ae33",
   "metadata": {},
   "source": [
    "Note that while MultiServerMCPClient can also connect using 'stdio' transport mode, it does not appear to work in Jupyter.  An example is shown below, and will run in python outside of Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e223eed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdio_client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"time\": {\n",
    "            \"transport\": \"stdio\",\n",
    "            \"command\": \"uv\",\n",
    "            \"args\": [\"run\",\"C:/Users/Andrew/Documents/dkit-projects/agentic-labs/MCP/get-time.py\"],\n",
    "        }\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9105ce6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StructuredTool(name='get_current_time', description='Get the current time in ISO 8601 format.', args_schema={'properties': {}, 'title': 'get_current_timeArguments', 'type': 'object'}, response_format='content_and_artifact', coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x000001CA0E7EA700>)]\n"
     ]
    }
   ],
   "source": [
    "# tools list from time server\n",
    "\n",
    "tools = await client.get_tools()\n",
    "print(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2446f47b",
   "metadata": {},
   "source": [
    "# Record Shop with MCP\n",
    "The example below ca remote server for tools.\n",
    "Since MCP tool call is async, all invoke calls must be async as well.\n",
    "\n",
    "llm.invoke(...) becomes:  await llm.ainvoke(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3ad96f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Tom,\n",
      "\n",
      "Yes, we have several David Bowie albums available! Here's what we have in stock:\n",
      "\n",
      "*   **The Rise and Fall of Ziggy Stardust and the Spiders From Mars** (1972) - Rock, Classic Rock, Glam\n",
      "*   **Hunky Dory** (1971) - Rock, Classic Rock, Glam\n",
      "*   **Low** (1977) - Electronic, Rock, Art Rock, Ambient, Experimental\n",
      "*   **Aladdin Sane** (1973) - Rock, Glam\n",
      "*   **Station to Station** (1976) - Rock, Funk / Soul, Classic Rock, Soul, Funk, Art Rock\n",
      "\n",
      "Let me know if you'd like more information on any of these or if you're looking for something else!\n",
      "\n",
      "Cheers,\n",
      "Andrew\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Tom,\\n\\nYes, we have several David Bowie albums available! Here's what we have in stock:\\n\\n*   **The Rise and Fall of Ziggy Stardust and the Spiders From Mars** (1972) - Rock, Classic Rock, Glam\\n*   **Hunky Dory** (1971) - Rock, Classic Rock, Glam\\n*   **Low** (1977) - Electronic, Rock, Art Rock, Ambient, Experimental\\n*   **Aladdin Sane** (1973) - Rock, Glam\\n*   **Station to Station** (1976) - Rock, Funk / Soul, Classic Rock, Soul, Funk, Art Rock\\n\\nLet me know if you'd like more information on any of these or if you're looking for something else!\\n\\nCheers,\\nAndrew\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--e70637b8-5e15-43a9-ae89-4f033ed64f9e-0', usage_metadata={'input_tokens': 700, 'output_tokens': 180, 'total_tokens': 880, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "# import HumanMessages for the LLM invocation\n",
    "from langchain.messages import HumanMessage, SystemMessage\n",
    "# import checkpointer\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "from langchain.messages import AnyMessage, RemoveMessage\n",
    "\n",
    "from langgraph.prebuilt import ToolNode, InjectedState\n",
    "from langchain.tools import tool, ToolRuntime\n",
    "from langchain_community.tools.file_management.read import ReadFileTool\n",
    "\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from langgraph.types import interrupt, Command\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "mcp_client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"music_catalog_srv\": {\n",
    "            \"transport\": \"streamable_http\",\n",
    "            \"url\": \"http://localhost:8001/mcp\",\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "tools = await mcp_client.get_tools()\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=os.getenv(\"GOOGLE_API_MODEL\"),\n",
    "    temperature=0)\n",
    "\n",
    "class RecordShopState(MessagesState):\n",
    "    email: str\n",
    "    success: bool\n",
    "    count: int = 0\n",
    "\n",
    "async def action(state: RecordShopState) -> dict:\n",
    "    result = await llm.bind_tools(tools).ainvoke(state['messages'])\n",
    "    return {\"messages\": result}\n",
    "\n",
    "graph = StateGraph(RecordShopState)\n",
    "graph.add_node(\"action\", action)\n",
    "graph.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "graph.add_edge(START, \"action\")\n",
    "# conditional edge - if tool_calls == [] for to END else to \"tools\"\n",
    "graph.add_conditional_edges(\"action\", \n",
    "    lambda state: state['messages'][-1].tool_calls == [],\n",
    "    path_map={True: END, False: \"tools\"}\n",
    "    )\n",
    "graph.add_edge(\"tools\", \"action\")\n",
    "\n",
    "compiled_graph = graph.compile()\n",
    "    \n",
    "\n",
    "system_prompt_react = \"\"\"\n",
    "You are a helpful assistant in a record shop that sells albums.  \n",
    "You are answering queries from emails about what albums are available or making recommendations.\n",
    "Answer the queries in a friendly and informative way.  If the email includes the customer name please use it. Sign your emails in the name of Andrew.\n",
    "Think step by step and call the tools as needed to get the information required to answer the query.\n",
    "\n",
    "\"\"\"\n",
    "prompt_react = \"\"\"\n",
    "\n",
    "\n",
    "The query is:\n",
    "{query}\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"emails.csv\")\n",
    "for index, row in df.iterrows():\n",
    "    email = row['email']\n",
    "    run = row['run']\n",
    "    if run == \"yes\":\n",
    "        messages = [SystemMessage(content=system_prompt_react),\n",
    "                    HumanMessage(content=prompt_react.format(query=email))]\n",
    "        response = await compiled_graph.ainvoke(input=RecordShopState(messages=messages, success=True, count=0, email=email), config=RunnableConfig(recursion_limit=100))\n",
    "        # response = main_react(email)\n",
    "        df.at[index, 'response'] = response['messages'  ][-1].content\n",
    "        print(response['messages'][-1].content)\n",
    "# response['messages'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cee478",
   "metadata": {},
   "source": [
    "## Connecting to Tavily Remote MCP server\n",
    "\n",
    "We can connect to vendor hosted MCP servers using the remote HTTP URL.\n",
    "\n",
    "Below is an example using the Tavily hosted MCP server.  We can get the list of tools and execute queries all through the remote Cloud environment.  Note, you will need a Tavily API key to perform this connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8eac63eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tavily_api_key = os.getenv(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ada650e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tavily_server = MultiServerMCPClient(\n",
    "    {\n",
    "        \"tavily_srv\": {\n",
    "            \"transport\": \"streamable_http\",\n",
    "            \"url\": f\"https://mcp.tavily.com/mcp/?tavilyApiKey={tavily_api_key}\",\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2204f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = await tavily_server.get_tools()\n",
    "print(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f0d329b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"MCP\" is an acronym that can stand for several different things, depending on the context. To help you find what you're looking for, here are some of the most common meanings and related web pages:\n",
      "\n",
      "1.  **Microsoft Certified Professional (MCP):** This is a very common meaning, referring to individuals who have passed certification exams for Microsoft products and technologies.\n",
      "    *   You'll find information on Microsoft's official learning and certification pages, as well as forums and training providers.\n",
      "    *   *Search terms:* \"Microsoft Certified Professional,\" \"MCP certification,\" \"Microsoft exams\"\n",
      "\n",
      "2.  **Monocalcium Phosphate (MCP):** A chemical compound often used as a leavening agent in baking (e.g., in baking powder) or as a feed additive for livestock.\n",
      "    *   You'd find information on food science sites, chemical suppliers, and agricultural resources.\n",
      "    *   *Search terms:* \"Monocalcium Phosphate,\" \"MCP food additive,\" \"MCP chemical\"\n",
      "\n",
      "3.  **Master Control Program (MCP):** A term from the movie *Tron* (and its sequel), referring to a powerful artificial intelligence.\n",
      "    *   You'd find information on movie databases, fan wikis, and entertainment sites.\n",
      "    *   *Search terms:* \"Master Control Program Tron,\" \"MCP Tron movie\"\n",
      "\n",
      "4.  **Metacarpophalangeal Joint (MCP Joint):** In anatomy, this refers to the joints in the hand where the metacarpal bones meet the phalanges (finger bones).\n",
      "    *   You'd find information on medical websites, anatomy resources, and orthopedic sites.\n",
      "    *   *Search terms:* \"Metacarpophalangeal joint,\" \"MCP joint anatomy,\" \"hand anatomy MCP\"\n",
      "\n",
      "5.  **Methylcyclopentadienyl Manganese Tricarbonyl (MMT or MCP):** An organometallic compound used as a fuel additive.\n",
      "    *   You'd find information on chemical industry sites, environmental agencies (regarding its use in fuel), and scientific journals.\n",
      "    *   *Search terms:* \"Methylcyclopentadienyl Manganese Tricarbonyl,\" \"MMT fuel additive,\" \"MCP fuel\"\n",
      "\n",
      "**To get more specific results, please tell me which \"MCP\" you are interested in!**\n"
     ]
    }
   ],
   "source": [
    "# import MessagesState\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "async def action(state: MessagesState) -> MessagesState:\n",
    "    result = await llm.ainvoke(state['messages'])\n",
    "    return {\"messages\": result}\n",
    "\n",
    "graph = StateGraph(MessagesState)\n",
    "graph.add_node(\"action\", action)\n",
    "graph.add_node(\"tools\", ToolNode(tools))\n",
    "graph.add_edge(START, \"action\")\n",
    "graph.add_conditional_edges(\"action\", \n",
    "    lambda state: state['messages'][-1].tool_calls == [],\n",
    "    path_map={True: END, False: \"tools\"}\n",
    "    )\n",
    "graph.add_edge(\"tools\", \"action\")\n",
    "compiled_graph = graph.compile()\n",
    "\n",
    "messages = [SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "            HumanMessage(content=\"Search for web pages on MCP\")]\n",
    "response = await compiled_graph.ainvoke(input=MessagesState(messages=messages))\n",
    "print(response['messages'][-1].content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf15b3d3",
   "metadata": {},
   "source": [
    "## Using MCP Injection\n",
    "\n",
    "This is value for allowing arguments to be sent to tool without needing to go to model.\n",
    "\n",
    "However, appears to have issues in environment or release at present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afb8f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langchain_mcp_adapters.interceptors import MCPToolCallRequest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9d717d",
   "metadata": {},
   "source": [
    "# Notes on Running MCP Servers\n",
    "\n",
    "Example servers in the MCP folder on Moodle should be run on the command line, prefferably with UV. For instance:\n",
    ">  uv run get-time-svr.py\n",
    "\n",
    "MCP servers can be tested through a web front end by using the MCP Inspector tool, found at:\n",
    "- https://github.com/modelcontextprotocol/inspector"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic-labs (3.13.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
